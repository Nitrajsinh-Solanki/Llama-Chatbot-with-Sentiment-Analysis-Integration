{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#                  **Wellness Bot with Sentiment Analysis**\n",
        "**Overview:-**\n",
        "\n",
        "This project builds a chatbot that adapts its responses based on user sentiment, providing empathetic and personalized replies. It uses Hugging Face models for both sentiment analysis and text generation and is deployed with Gradio for user interaction.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "**Sentiment Analysis:** DistilBERT identifies positive or negative sentiment in user input.\n",
        "\n",
        "**Text Generation:** Llama-2 model generates conversational responses based on input and sentiment.\n",
        "\n",
        "**Dynamic Responses:** The bot adjusts its tone, offering empathy for negative sentiment and encouragement for positive sentiment.\\\n",
        "**Interactive Interface:** Gradio provides a simple text-based interface for users to chat with the bot. The conversation history can be reset with the \"clear\" command.\n",
        "\n",
        "**Purpose:-**\n",
        "The chatbot is designed for applications like mental health support, customer service, and education by offering sentiment-aware, personalized conversations."
      ],
      "metadata": {
        "id": "43kX24SzKoiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Install Necessary Libraries**\n",
        "Install Required Libraries\n",
        "This step installs the necessary libraries including PyTorch, Hugging Face's transformers,Gradio (for building the user interface), and sentence-transformers for sentiment analysis."
      ],
      "metadata": {
        "id": "pfMYwzyHLmrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio transformers sentencepiece gradio sentence-transformers accelerate"
      ],
      "metadata": {
        "id": "cCzy3b1UL5Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Import Required Libraries**\n",
        "Importing PyTorch and Hugging Face modules for working with models and Gradio for creating the chatbot interface."
      ],
      "metadata": {
        "id": "qP8SRYE5L_e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "zD7opIaLMWC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3:Sentiment Analysis Pipeline**\n",
        "Initialize Sentiment Analysis Pipeline\n",
        "Using a pre-trained **DistilBERT** model to perform sentiment analysis. This model is fine-tuned on the SST-2 dataset,\n",
        "which is designed for sentiment classification tasks (positive or negative sentiment)."
      ],
      "metadata": {
        "id": "vMzK_pbOMn-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
      ],
      "metadata": {
        "id": "LYM4NNmYMxhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4:Load Llama-2 Model and Tokenizer**\n",
        "Load Llama Model for Text Generation\n",
        "The **Llama-2-7b-chat-hf model** from Hugging Face is loaded to handle conversational text generation.\n",
        "The model is loaded in half-precision (float16) for faster computation, and the device_map is set to \"auto\" to automatically\n",
        "use available hardware (such as GPUs, if available)"
      ],
      "metadata": {
        "id": "hQ8tfjfvM0Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(llama_model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)"
      ],
      "metadata": {
        "id": "8dO7li53M-ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5:Text Generation Pipeline**\n",
        "Initialize Text Generation Pipeline\n",
        "Setting up a text generation pipeline using the Llama model for generating responses.\n",
        "`max_length` is set to 4096 to handle longer conversations, `do_sample=True` allows for sampling when generating text,\n",
        "and `temperature` and `top_p` control the randomness and diversity of the generated responses."
      ],
      "metadata": {
        "id": "S_JfRx5QNByu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_generator = pipeline(\n",
        "    \"text-generation\", model=llama_model, tokenizer=llama_tokenizer,\n",
        "    torch_dtype=torch.float16, device_map=\"auto\",\n",
        "    max_length=4096, do_sample=True, temperature=0.6, top_p=0.95\n",
        ")\n"
      ],
      "metadata": {
        "id": "ET0iK5EONK_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6:Generate Bot's Response**\n",
        " Define Function to Generate Response\n",
        "The function `generate_response` generates a response based on the user input. The maximum response length is dynamically\n",
        "calculated based on the length of the user's input to ensure appropriate response lengths."
      ],
      "metadata": {
        "id": "8Lb3RfU-NNtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input):\n",
        "    input_len = len(user_input.split())\n",
        "    max_len = min(4096, input_len * 10 )\n",
        "    return response_generator(user_input, max_length=max_len, do_sample=True)[0]['generated_text'].strip()"
      ],
      "metadata": {
        "id": "-nkIPWc2NWs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Analyze Sentiment of User Input**\n",
        "Define Function for Sentiment Analysis\n",
        "This function analyzes the sentiment of the user's input using the sentiment analysis pipeline.\n",
        "Based on the sentiment (positive or negative) and its confidence score, it returns a relevant response.\n",
        "If the sentiment is very negative, the chatbot responds with an empathetic message.\n",
        "If positive, it encourages the user, and if neutral or low confidence, it gives a general supportive message"
      ],
      "metadata": {
        "id": "C8GWiJvTNapn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(user_input):\n",
        "    sentiment = sentiment_analyzer(user_input)[0]\n",
        "    label, score = sentiment['label'], sentiment['score']\n",
        "    if label == \"NEGATIVE\":\n",
        "        return \"Sorry you're feeling down. I'm here for you.\" if score > 0.9 else \"It seems rough. I'm listening.\"\n",
        "    elif label == \"POSITIVE\":\n",
        "        return \"Great to hear! Keep up the good vibes!\" if score > 0.9 else \"Glad you're feeling better.\"\n",
        "    else:\n",
        "        return \"I'm here for you no matter what.\""
      ],
      "metadata": {
        "id": "K8eIMmV-NjSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8:Clean Redundant Phrases from Generated Text**\n",
        " Define Function to Clean Up Redundant Phrases\n",
        "After generating a response, this function checks for redundant or overused phrases and removes them\n",
        "to keep the conversation fresh and avoid repetitive replies."
      ],
      "metadata": {
        "id": "5mug6MKhN4PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_redundant(user_input, generated_text):\n",
        "    phrases_to_remove = [\"It seems rough.\", \"Take a deep breath.\"]\n",
        "    for phrase in phrases_to_remove:\n",
        "        generated_text = generated_text.replace(phrase, \"\").strip()\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "SdCnfI4YN_WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9:Main Chatbot Reply Logic**\n",
        "Define Function for Chatbot Reply\n",
        "This function combines sentiment analysis with text generation to form a chatbot response.\n",
        "First, it analyzes the sentiment of the user input, generates a response based on that input,\n",
        "and then cleans up any redundant phrases before returning the final chatbot response."
      ],
      "metadata": {
        "id": "JkA-9n8wOA2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3qEyNL2wLB8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_reply(user_input):\n",
        "    sentiment_msg = analyze_sentiment(user_input)\n",
        "    generated_text = generate_response(user_input)\n",
        "    cleaned_response = clean_redundant(user_input, generated_text)\n",
        "    return f\"{sentiment_msg} {cleaned_response}\".strip()"
      ],
      "metadata": {
        "id": "j1iMBJHEOIuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10:Track Chat History and Handle \"Clear\" Command**\n",
        " Initialize Chat History\n",
        "This list stores the conversation history between the user and the chatbot. It is updated with every new user input."
      ],
      "metadata": {
        "id": "1ez69j1yOJ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []"
      ],
      "metadata": {
        "id": "IZd2JKL3OQNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11:Track Chat History and Handle 'Clear' Command**\n",
        "Define Function for Chat Interface\n",
        "This function manages the interface by interacting with the user input.\n",
        "It handles special commands like 'clear' to reset the conversation history, generates the chatbot's reply,\n",
        "and appends the conversation to the chat history"
      ],
      "metadata": {
        "id": "-UI9YX8DOS12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interface_function(user_input):\n",
        "    global chat_history\n",
        "    if user_input.lower() == \"clear\":\n",
        "        chat_history = []\n",
        "        return \"Chat history cleared.\"\n",
        "\n",
        "    reply = chatbot_reply(user_input)\n",
        "    chat_history.append(f\"USER: {user_input}\\nBOT: {reply}\")\n",
        "    return \"\\n\\n\".join(chat_history)"
      ],
      "metadata": {
        "id": "ImVuyN5COZnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12:Launch Gradio Interface**\n",
        " Set Up Gradio Interface\n",
        "Gradio is used to create a simple and interactive web interface for the chatbot. The interface consists of\n",
        "a text input for the user to type their messages, and it displays the chatbot's replies based on the sentiment analysis."
      ],
      "metadata": {
        "id": "BTJuMvVuObry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(\n",
        "    fn=interface_function,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Wellness Bot\",\n",
        "    description=\"Chat about your feelings, and the bot will respond based on sentiment. Type 'clear' to clear chat.\",\n",
        "    allow_flagging=\"never\"\n",
        ")"
      ],
      "metadata": {
        "id": "XIch3SdaOkV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 13: Launch Gradio Interface**\n",
        "Launch the Interface\n",
        "This launches the Gradio interface, allowing users to interact with the chatbot in a web-based environment.\n",
        "The `debug=True` flag helps in identifying issues if the code encounters any errors during execution."
      ],
      "metadata": {
        "id": "zWjqctn6OoEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "D8ZBE1XmOvGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}